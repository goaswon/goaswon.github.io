---
title:  "Article5. 불균형 데이터 처리 2125"

categories:
  - 빅데이터 분석 기사
tags: 
  - Part2. 빅데이터 탐색
  - Chapter1. 데이터 전처리
  - Section2. 분석 변수 처리
  - Article5. 불균형 데이터 처리

toc: true
toc_sticky: true
 
date: 2021-02-23
last_modified_at: 2021-02-25
---

Section2. [분석 변수 처리]()

- 탐색하는 타깃 데이터의 수가 매우 극소수인 경우에 불균형 데이터 처리를 한다.
- 불균형 데이터 처리 기법으로는 언더 샘플링, 오버 샘플링, 임곗값 이동(Threshold Moving), 앙상블(Ensemble) 기법이 있다.

> 예)
>
> 건물에서 화재가 발생할 확률은 1% 이하이다. 이와 같이 불균형 데이터에서는 정확도(Accuracy)가 높아도 재현율(Recall)이 급격히 작아지는 현상이 발생한다. 100개의 데이터 중 1개가 화재임ㄴ, 모두 정상으로 예측해도 정확도가 99%이다.

## Subparagraph1. 언더 샘플링(Under-Sampling)

- 언더 샘플링은 다수 클래스의 데이터를 일부만 선택하여 데이터의 비율을 맞추는 방법이다.
- 언더 샘플링의 경우 데이터의 소실이 매우 크고, 때로는 중요한 정상 데이터를 잃을 수 있다.
- 언더 샘플링의 대표적인 기법에는 랜덤 언더 샘플링, ENN, 토멕링크 방법, CNN, OSS 등이 있다.

⬇언더 샘플링의 기법

| 기법                                    | 설명                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| 랜덤 언더 샘플링(Random Under-Sampling) | • 무작위로 다수 클래스 데이터의 일부만 선택하는 방법         |
| ENN(Edited Nearest Neighbours)          | • 소스 클래스 주위에 인접한 다수 클래스 데이터를 제거하여 데이터의 비율을 맞추는 방법 |
| 토멕 링크 방법(Tomek Link Method)       | • 토멕 링크는 클래스를 구분하는 경계선 가까이에 존재하는 데이터<br />• 다수 클래스에 속한 토멕 링크를 제거하는 방법 |
| CNN(Condensed Nearest Neighbour)        | • 다수 클래스에 밀집한 데이터가 없을 때까지 데이터를 제거하여 데이터 분포에서 대표적인 데이터만 남도록 하는 방법 |
| OSS(One Sided Seletion)                 | • 토멕 링크 방법과 Condensed Nearest Neighbour 기법의 장점을 섞은 방법<br />• 다수 클래스의 데이터를 토멕 링크 방법으로 제거한 후 Condensed Nearest Neighbor를 이용하여 밀집된 데이터 제거 |

![언더 샘플링](https://mkjjo.github.io/img/posting/2019-01-04-001-undersampling.PNG)

## Subparagraph2. 오버 샘플링(Over-Sampling)

- 오버 샘플링은 소수 클래스의 데이터를 복제 또는 생성하여 데이터의 비율을 맞추는 방법으로 과대 샘플링이라고도 한다.
- 정보가 손실되지 않는다는 장점이 있으나 과적합(Over-fitting)을 초래할 수 있다.
- 알고리즘의 성능은 높으나 검증의 성능은 나빠질 수 있다.
- 오버 샘플링의 대표적인 기법에는 랜덤 오버 샘플링, SMOTE, Borderline-SMOTE, ADASYN 등이 있다.

⬇오버 샘플링의 기법

| 기법                                              | 설명                                                         |
| ------------------------------------------------- | ------------------------------------------------------------ |
| 랜덤 오버 샘플링(Random Over-Sampling)            | • 무작위로 소수 클래스 데이터를 복제하여 데이터의 비율을 맞추는 방법 |
| SMOTE(Synthetic Minority Over-sampling TEchnique) | • 소수 클래스에서 중심이 되는 데이터와 주변 데이터 사이에 가상의 직선을 만든 후, 그 위에 데이터를 추가하는 방법 |
| Borderline-SMOTE                                  | • 다수 클래스와 소수 클래스의 경계선에서 SMOTE를 적용하는 방법 |
| ADASYN(ADAptive SYNthetic)                        | • 모든 소수 클래스에서 다수 클래스의 관측비율을 계산하여 SMOTE를 적용하는 방법 |

- 언더 샘플링, 오버 샘플링 이외에 불균형 데이터 처리 기법으로는 임곗값 이동(Threshold moving), 앙상블(Ensemble) 기법이 있다.

![오버 샘플링](https://mkjjo.github.io/img/posting/2019-01-04-001-oversampling.PNG)

## Subparagraph3. 임곗값 이동(Threshold-Moving)

- 임곗값 이동은 임곗값을 데이터가 많은 쪽으로 이동시키는 방법이다.
- 학습 단계에서는 변화 없이 학습하고 테스트 단계에서 임곗값을 이동한다.

> 예)
>
> 양성 90개, 음성 10개의 데이터 세트 샘플이 있다.
>
> | 방법        | 설명                                                       |
> | ----------- | ---------------------------------------------------------- |
> | 언더 샘플링 | 양성 클래스의 샘플을 10개로 만들기                         |
> | 오버 샘플링 | 음성 클래스의 샘플을 90개로 만들기                         |
> | 임곗값 이동 | 분류 시행할 때 사용되는 임곗값을 양성과 음성의 비율로 조정 |

## Subparagraph4. 앙상블 기법(Ensemble Technique)

- 앙상블은 같거나 서로 다른 여러 가지 모형들의 예측/분류 결과를 종합하여 최종적인 의사결정에 활용하는 기법이다.
- 오버 샘플링, 언더 샘플링, 임곗값 이동을 조합한 앙상블을 만들 수 있다.
- 앙상블의 예측 중에서 가장 많은 표를 받은 클래스를 최종적으로 선택한다.

